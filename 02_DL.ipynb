{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0VpkdMgHRrx",
        "outputId": "482052f6-c390-4211-947b-1b2b86fd6bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Model: LeNet-5\n",
            "Number of parameters: 62006\n",
            "[Epoch 1, Batch 200] Loss: 2.303\n",
            "[Epoch 1, Batch 400] Loss: 2.292\n",
            "[Epoch 1, Batch 600] Loss: 2.264\n",
            "[Epoch 2, Batch 200] Loss: 2.102\n",
            "[Epoch 2, Batch 400] Loss: 2.037\n",
            "[Epoch 2, Batch 600] Loss: 1.971\n",
            "[Epoch 3, Batch 200] Loss: 1.875\n",
            "[Epoch 3, Batch 400] Loss: 1.827\n",
            "[Epoch 3, Batch 600] Loss: 1.764\n",
            "[Epoch 4, Batch 200] Loss: 1.698\n",
            "[Epoch 4, Batch 400] Loss: 1.662\n",
            "[Epoch 4, Batch 600] Loss: 1.660\n",
            "[Epoch 5, Batch 200] Loss: 1.615\n",
            "[Epoch 5, Batch 400] Loss: 1.588\n",
            "[Epoch 5, Batch 600] Loss: 1.563\n",
            "[Epoch 6, Batch 200] Loss: 1.544\n",
            "[Epoch 6, Batch 400] Loss: 1.522\n",
            "[Epoch 6, Batch 600] Loss: 1.516\n",
            "[Epoch 7, Batch 200] Loss: 1.466\n",
            "[Epoch 7, Batch 400] Loss: 1.476\n",
            "[Epoch 7, Batch 600] Loss: 1.447\n",
            "[Epoch 8, Batch 200] Loss: 1.418\n",
            "[Epoch 8, Batch 400] Loss: 1.419\n",
            "[Epoch 8, Batch 600] Loss: 1.407\n",
            "[Epoch 9, Batch 200] Loss: 1.380\n",
            "[Epoch 9, Batch 400] Loss: 1.369\n",
            "[Epoch 9, Batch 600] Loss: 1.369\n",
            "[Epoch 10, Batch 200] Loss: 1.347\n",
            "[Epoch 10, Batch 400] Loss: 1.325\n",
            "[Epoch 10, Batch 600] Loss: 1.315\n",
            "Accuracy on test set: 52.48%\n",
            "-----------------------------------\n",
            "Model: VGG-16\n",
            "Number of parameters: 33638218\n",
            "[Epoch 1, Batch 200] Loss: 2.303\n",
            "[Epoch 1, Batch 400] Loss: 2.303\n",
            "[Epoch 1, Batch 600] Loss: 2.303\n",
            "[Epoch 2, Batch 200] Loss: 2.303\n",
            "[Epoch 2, Batch 400] Loss: 2.303\n",
            "[Epoch 2, Batch 600] Loss: 2.303\n",
            "[Epoch 3, Batch 200] Loss: 2.303\n",
            "[Epoch 3, Batch 400] Loss: 2.303\n",
            "[Epoch 3, Batch 600] Loss: 2.303\n",
            "[Epoch 4, Batch 200] Loss: 2.303\n",
            "[Epoch 4, Batch 400] Loss: 2.303\n",
            "[Epoch 4, Batch 600] Loss: 2.303\n",
            "[Epoch 5, Batch 200] Loss: 2.303\n",
            "[Epoch 5, Batch 400] Loss: 2.303\n",
            "[Epoch 5, Batch 600] Loss: 2.303\n",
            "[Epoch 6, Batch 200] Loss: 2.303\n",
            "[Epoch 6, Batch 400] Loss: 2.303\n",
            "[Epoch 6, Batch 600] Loss: 2.303\n",
            "[Epoch 7, Batch 200] Loss: 2.303\n",
            "[Epoch 7, Batch 400] Loss: 2.303\n",
            "[Epoch 7, Batch 600] Loss: 2.303\n",
            "[Epoch 8, Batch 200] Loss: 2.303\n",
            "[Epoch 8, Batch 400] Loss: 2.303\n",
            "[Epoch 8, Batch 600] Loss: 2.303\n",
            "[Epoch 9, Batch 200] Loss: 2.303\n",
            "[Epoch 9, Batch 400] Loss: 2.303\n",
            "[Epoch 9, Batch 600] Loss: 2.303\n",
            "[Epoch 10, Batch 200] Loss: 2.303\n",
            "[Epoch 10, Batch 400] Loss: 2.303\n",
            "[Epoch 10, Batch 600] Loss: 2.303\n",
            "Accuracy on test set: 10.45%\n",
            "-----------------------------------\n",
            "Model: ResNet-18\n",
            "Number of parameters: 4737098\n",
            "[Epoch 1, Batch 200] Loss: 1.647\n",
            "[Epoch 1, Batch 400] Loss: 1.365\n",
            "[Epoch 1, Batch 600] Loss: 1.236\n",
            "[Epoch 2, Batch 200] Loss: 1.037\n",
            "[Epoch 2, Batch 400] Loss: 1.006\n",
            "[Epoch 2, Batch 600] Loss: 0.959\n",
            "[Epoch 3, Batch 200] Loss: 0.815\n",
            "[Epoch 3, Batch 400] Loss: 0.806\n",
            "[Epoch 3, Batch 600] Loss: 0.812\n",
            "[Epoch 4, Batch 200] Loss: 0.649\n",
            "[Epoch 4, Batch 400] Loss: 0.685\n",
            "[Epoch 4, Batch 600] Loss: 0.704\n",
            "[Epoch 5, Batch 200] Loss: 0.536\n",
            "[Epoch 5, Batch 400] Loss: 0.547\n",
            "[Epoch 5, Batch 600] Loss: 0.591\n",
            "[Epoch 6, Batch 200] Loss: 0.446\n",
            "[Epoch 6, Batch 400] Loss: 0.474\n",
            "[Epoch 6, Batch 600] Loss: 0.488\n",
            "[Epoch 7, Batch 200] Loss: 0.345\n",
            "[Epoch 7, Batch 400] Loss: 0.365\n",
            "[Epoch 7, Batch 600] Loss: 0.409\n",
            "[Epoch 8, Batch 200] Loss: 0.294\n",
            "[Epoch 8, Batch 400] Loss: 0.303\n",
            "[Epoch 8, Batch 600] Loss: 0.347\n",
            "[Epoch 9, Batch 200] Loss: 0.233\n",
            "[Epoch 9, Batch 400] Loss: 0.251\n",
            "[Epoch 9, Batch 600] Loss: 0.264\n",
            "[Epoch 10, Batch 200] Loss: 0.169\n",
            "[Epoch 10, Batch 400] Loss: 0.188\n",
            "[Epoch 10, Batch 600] Loss: 0.225\n",
            "Accuracy on test set: 71.25%\n",
            "-----------------------------------\n",
            "Model: DenseNet-121\n",
            "Number of parameters: 6964106\n",
            "[Epoch 1, Batch 200] Loss: 1.983\n",
            "[Epoch 1, Batch 400] Loss: 1.676\n",
            "[Epoch 1, Batch 600] Loss: 1.570\n",
            "[Epoch 2, Batch 200] Loss: 1.409\n",
            "[Epoch 2, Batch 400] Loss: 1.371\n",
            "[Epoch 2, Batch 600] Loss: 1.330\n",
            "[Epoch 3, Batch 200] Loss: 1.203\n",
            "[Epoch 3, Batch 400] Loss: 1.205\n",
            "[Epoch 3, Batch 600] Loss: 1.176\n",
            "[Epoch 4, Batch 200] Loss: 1.064\n",
            "[Epoch 4, Batch 400] Loss: 1.072\n",
            "[Epoch 4, Batch 600] Loss: 1.058\n",
            "[Epoch 5, Batch 200] Loss: 0.948\n",
            "[Epoch 5, Batch 400] Loss: 0.931\n",
            "[Epoch 5, Batch 600] Loss: 0.950\n",
            "[Epoch 6, Batch 200] Loss: 0.829\n",
            "[Epoch 6, Batch 400] Loss: 0.844\n",
            "[Epoch 6, Batch 600] Loss: 0.856\n",
            "[Epoch 7, Batch 200] Loss: 0.731\n",
            "[Epoch 7, Batch 400] Loss: 0.750\n",
            "[Epoch 7, Batch 600] Loss: 0.779\n",
            "[Epoch 8, Batch 200] Loss: 0.639\n",
            "[Epoch 8, Batch 400] Loss: 0.675\n",
            "[Epoch 8, Batch 600] Loss: 0.688\n",
            "[Epoch 9, Batch 200] Loss: 0.564\n",
            "[Epoch 9, Batch 400] Loss: 0.577\n",
            "[Epoch 9, Batch 600] Loss: 0.595\n",
            "[Epoch 10, Batch 200] Loss: 0.500\n",
            "[Epoch 10, Batch 400] Loss: 0.507\n",
            "[Epoch 10, Batch 600] Loss: 0.527\n",
            "Accuracy on test set: 63.82%\n",
            "-----------------------------------\n",
            "Model: MobileNetV2\n",
            "Number of parameters: 2236682\n",
            "[Epoch 1, Batch 200] Loss: 2.280\n",
            "[Epoch 1, Batch 400] Loss: 2.068\n",
            "[Epoch 1, Batch 600] Loss: 1.934\n",
            "[Epoch 2, Batch 200] Loss: 1.742\n",
            "[Epoch 2, Batch 400] Loss: 1.707\n",
            "[Epoch 2, Batch 600] Loss: 1.645\n",
            "[Epoch 3, Batch 200] Loss: 1.566\n",
            "[Epoch 3, Batch 400] Loss: 1.548\n",
            "[Epoch 3, Batch 600] Loss: 1.530\n",
            "[Epoch 4, Batch 200] Loss: 1.459\n",
            "[Epoch 4, Batch 400] Loss: 1.465\n",
            "[Epoch 4, Batch 600] Loss: 1.430\n",
            "[Epoch 5, Batch 200] Loss: 1.373\n",
            "[Epoch 5, Batch 400] Loss: 1.361\n",
            "[Epoch 5, Batch 600] Loss: 1.363\n",
            "[Epoch 6, Batch 200] Loss: 1.315\n",
            "[Epoch 6, Batch 400] Loss: 1.318\n",
            "[Epoch 6, Batch 600] Loss: 1.286\n",
            "[Epoch 7, Batch 200] Loss: 1.263\n",
            "[Epoch 7, Batch 400] Loss: 1.246\n",
            "[Epoch 7, Batch 600] Loss: 1.264\n",
            "[Epoch 8, Batch 200] Loss: 1.203\n",
            "[Epoch 8, Batch 400] Loss: 1.216\n",
            "[Epoch 8, Batch 600] Loss: 1.225\n",
            "[Epoch 9, Batch 200] Loss: 1.154\n",
            "[Epoch 9, Batch 400] Loss: 1.162\n",
            "[Epoch 9, Batch 600] Loss: 1.188\n",
            "[Epoch 10, Batch 200] Loss: 1.126\n",
            "[Epoch 10, Batch 400] Loss: 1.137\n",
            "[Epoch 10, Batch 600] Loss: 1.132\n",
            "Accuracy on test set: 55.32%\n",
            "-----------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transforms and load CIFAR-10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Function to count the number of parameters in a model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# LeNet-5 architecture\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# VGG-16 architecture\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 1 * 1, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# ResNet-18 architecture\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(64, 64, 2)\n",
        "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, 10)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# DenseNet-121 architecture\n",
        "class DenseNet121(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNet121, self).__init__()\n",
        "        self.features = torchvision.models.densenet121(pretrained=False).features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x.relu_()\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# MobileNetV2 architecture\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MobileNetV2, self).__init__()\n",
        "        self.model = torchvision.models.mobilenet_v2(pretrained=False)\n",
        "        self.model.classifier[-1] = nn.Linear(1280, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "# Define the models\n",
        "models = {\n",
        "    \"LeNet-5\": LeNet5(),\n",
        "    \"VGG-16\": VGG16(),\n",
        "    \"ResNet-18\": ResNet18(),\n",
        "    \"DenseNet-121\": DenseNet121(),\n",
        "    \"MobileNetV2\": MobileNetV2()\n",
        "}\n",
        "\n",
        "# Training and evaluation loop\n",
        "for model_name, model in models.items():\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Number of parameters: {count_parameters(model)}\")\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(10):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 200 == 199:\n",
        "                print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 200:.3f}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "    # Evaluation\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy on test set: {accuracy:.2f}%\")\n",
        "    print(\"-----------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QtRg1S_1IaMv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}